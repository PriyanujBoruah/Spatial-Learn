Module 1: Introduction to Linear Regression and the SGDRegressor ðŸ“ˆ
This module introduces the fundamentals of linear regression, how to establish a baseline, and the powerful SGDRegressor for large-scale learning.

Baseline Models (DummyRegressor): Before building a complex model, it's a good practice to create a simple baseline. The DummyRegressor from scikit-learn serves this purpose by making predictions using simple strategies like the mean or median of the training data. This provides a reference point to evaluate whether your more complex model is actually learning meaningful patterns.

SGDRegressor Estimator: The primary focus of this week is the SGDRegressor, which implements Stochastic Gradient Descent (SGD). SGD is an iterative optimization algorithm that updates the model's parameters using one training sample at a time. This makes it highly efficient and scalable for very large datasets (e.g., > 10,000 samples) where fitting the entire dataset into memory might be computationally expensive.

Key Characteristics of SGDRegressor:

Sensitivity to Feature Scaling: Because SGD is sensitive to the scale of the features, it is crucial to perform feature scaling (e.g., using StandardScaler) before training the model to ensure that all features contribute equally to the learning process.

Stochastic Nature: The "stochastic" nature of SGD means that the model's parameters are updated for each training sample, leading to a more noisy but generally faster convergence compared to batch gradient descent.

Basic Implementation:

Python

from sklearn.linear_model import SGDRegressor

# Instantiate the regressor with a random state for reproducibility
linear_regressor = SGDRegressor(random_state=42)

# Train the model
linear_regressor.fit(X_train_scaled, y_train)