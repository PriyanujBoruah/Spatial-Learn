Module 2: Configuring MLP Architecture and Activation Functions ðŸ§±
This module focuses on the key hyperparameters that define the structure of the MLP and the non-linear functions that give it its power.

Defining the Network's Architecture (hidden_layer_sizes): This is one of the most important hyperparameters. It is a tuple where the length of the tuple determines the number of hidden layers, and each element in the tuple specifies the number of neurons in that layer. For example, hidden_layer_sizes=(15, 10, 5) creates an MLP with three hidden layers: the first with 15 neurons, the second with 10, and the third with 5.

Activation Functions (activation): The activation function introduces non-linearity into the model, allowing it to learn complex relationships. This function is applied to the output of each neuron in the hidden layers. Scikit-learn offers several choices:

'identity': A linear activation function (no-op), which essentially makes the model linear.

'logistic': The logistic sigmoid function, which squashes values to a range between 0 and 1.

'tanh': The hyperbolic tan function, which squashes values to a range between -1 and 1.

'relu' (Default): The Rectified Linear Unit function, which returns max(0, x). This is the most commonly used activation function in modern neural networks due to its efficiency and effectiveness.

Regularization (alpha): To prevent overfitting, MLPs in scikit-learn use L2 regularization, which adds a penalty to the loss function based on the square of the weights. The strength of this regularization is controlled by the alpha parameter. The default value is 0.0001.