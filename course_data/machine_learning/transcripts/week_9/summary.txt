Week 9 introduces two major topics: Multilayer Perceptron (MLP), a powerful supervised learning algorithm from the world of neural networks, and an introduction to unsupervised learning through Clustering, with a specific focus on Hierarchical Agglomerative Clustering (HAC).

The first part of the week is dedicated to understanding MLPs. These are non-linear function approximators that can be used for both classification and regression tasks. The week covers the implementation of MLPs in scikit-learn using MLPClassifier and MLPRegressor. Key aspects of configuring an MLP are detailed, including defining the network's architecture with the hidden_layer_sizes parameter, choosing an activation function for the hidden layers (such as 'relu' or 'tanh'), and applying L2 regularization using the alpha parameter. The module also explains the different optimization algorithms, or solvers ('lbfgs', 'sgd', 'adam'), used for training the network and how to inspect the trained model's weights and biases.

The second half of the week shifts to unsupervised learning, introducing the concept of clustering. The focus is on Hierarchical Agglomerative Clustering (HAC), a bottom-up approach that starts with each data point as its own cluster and progressively merges the closest clusters. The module explains the core algorithm and the different linkage criteria (single, complete, average, and Ward's) that determine how the distance between clusters is measured for merging.