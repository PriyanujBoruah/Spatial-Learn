Module 2: Probabilistic Clustering with Gaussian Mixture Models (GMM) ðŸŽ²
This module introduces a distribution-based approach to clustering, where clusters are modeled as a mixture of probability distributions.

Beyond Hard Assignments: Unlike K-Means, which assigns each point to a single cluster (hard clustering), a Gaussian Mixture Model (GMM) is a probabilistic model that provides a "soft" assignment. This means it calculates the probability that a data point belongs to each of the clusters.

The GMM Assumption: GMM assumes that the data points are generated from a mixture of several Gaussian distributions, each with its own mean and covariance. The goal is to find the parameters of these Gaussian distributions that best fit the data.

The Expectation-Maximization (EM) Algorithm: GMMs are typically trained using the EM algorithm, which is an iterative method for finding the maximum likelihood estimates of the parameters. It works in two steps:

Expectation (E-step): For each data point, calculate the probability that it was generated by each of the Gaussian components (the "soft" assignments).

Maximization (M-step): Update the parameters of each Gaussian component (mean, covariance, and mixing coefficient) based on the soft assignments from the E-step.
These two steps are repeated until the model's parameters converge.

Implementation in Scikit-Learn: The sklearn.mixture.GaussianMixture class is used to implement GMMs. A key hyperparameter is n_components, which specifies the number of Gaussian distributions to fit to the data.