So, let us calculate the weighted distance. So, first, we will specify the equation for the unweighted distance formula, then we will specify the weighted distance formula for the jth location from the ideal point. So, let us say option number j. Let us say what the coordinates of option j are. They are Yj1 and Yj, Yj2,. Let me write that properly, so what are the coordinates of each option here? Coordinates of each option here are Yj1 and Yj2, where j is for 1, 2, 3 or 4. And, what is my X? X is my X1 and X2, X1 and X2, so how is going to be the distance calculated between Yj and X? These are the two points, Yj is the coordinate of the jth location, X is the coordinate of the ideal location. How is the distance going to be calculated? Distance is going to be calculated using this Euclidean distance measure. This distance measure, how does that work? It is essentially Yj1 minus X1 whole square plus Yj2, Yj2 minus X2 whole square, and we will take a square root of this which is what is specified here. If there was a third dimension, I would have added Yj3 minus X3 whole square. I kept on adding more and more dimensions which is what this summation of P says. So, which is unweighted, there is no weight attached here. There is no weight attached, and we will do this for every option, so the distance of each option from the ideal location is given by this expression. Note here that I cannot calculate the numerical value for it as of now because I do not know the coordinates Xp. I do not know them. My objective is to find the coordinates, but I have a mathematical expression that will help me calculate the values of Xp. Let us attach weight to it. For each attribute P, let us attach weight to it, and this is my weighted distance now. So, for every dimension for every attribute, I will assign a weight, w1 is the weight attached by the consumer for attribute 1, w2 is the weight attached by the consumer for attribute 2, w3 is the weight attached by the consumer on attribute 3. For our problem, there is no attribute 3, so it does not matter, but w1 and w2 are the weights assigned by the consumer for each of the attributes. So, this time I will calculate the weighted distance between each option and the ideal point. Weights are all non-negative. Obviously, negative weights do not make sense; negative Xp does make sense, but negative weights do not make sense, so we will assume that all the weights are non-negative. So, all the Wp is greater than or equal to 0. However, Wp is equal to 0 is acceptable, which essentially means that the consumer is not providing any weight to that attribute when they are providing the choices. When this consumer is providing me with choices, she can say that this attribute is not important to me. Remember, in the car example, we said for some people, the color of the car is immaterial, so the consumer may say on that dimension instead of assigning 0 values to all the y's, because y's are not zeros, I will assign W to be 0. I will assign W to be 0. I will assign W to be 0. So, I do not care what the car is, or what the color of the car is. For that consumer, the weight assigned to colour attribute is 0, so 0 is an admissible value; negative values are not admissible. So, we will define Wp to be greater than or equal to 0. Now, let us define square distance, so the square root does not help me that much. I want to formulate a linear programming problem. Therefore, I need to get rid of all these power terms. So, let me get rid of this one and a half by defining what is called a square distance. So, let us Sj be the square distance, let Sj be the square distance. So, whatever the distance, I will square it so that the square root formula will go away, square root part will go away. Let me use the square distance. Now, let us look at the date of choice provided by the consumer. Where is my choice data? this is my choice rate. So, here the consumer is telling me that she prefers 2 over 3, between the choices 2 and 3, between the choices 2 and 4, she prefers between the choices 2 over 4 she prefers, that is what she prefers, that is what she prefers. So, let us come to that. So, this is the ordered pair. This is the ordered pair j, k where j denotes the preferred product, preferred product variant. And how does the consumer prefer j over k? Essentially, they are in their mind calculating the distance from the ideal location, so when the consumer says that she prefers j over k, essentially, it means that dj is less than dk; that is what she means; therefore, she prefers j or k, option j over option k, because she thinks that option j is closer to her ideal, then option k is to of the idea. So, essentially what we do not know, what we do not know are the weights and the ideal points. Those are the only variables. Everything else is data. Everything else is a parameter; everything else is a parameter, and they are known apriori. So, these are the only two variables, and therefore these become my decision variable, so these are my decision variables for the problem. These are my decision variables for the problem. Therefore, the optimization problem is to find W and X. W is a vector because I need to find weights for each attribute, so W1, W2, W3, all the way to Wt, t dimensional data. So, this is my vector W. What is my X? X is also X1, X2 all the way to Xt, t dimensional data, t dimensional data. So, I need to find the vector of weights assigned by the consumer to each of the attributes. X is the coordinate of the ideal point on each of these coordinates, each of the attributes. And what does it need to satisfy? It needs to satisfy this inequality. What inequality does it need to satisfy? that since the consumer prefers j over k since the consumer prefers j over k, that essentially means that Sj is smaller than or equal to Sk. I am not talking about dj and dk. I am talking about Sj and Sk. Square distances, but you know that if dj is less than dk, then obviously dj squared is less than dk square. So, we are assuming square distances. We are talking about square distances, so that is the inequality that we wish to satisfy. Now, if this is an optimization problem, there has to be an objective function. There has to be an objective function. What should be the objective function? We have the constraint. The only constraint is these pairwise comparisons have to be respected. Now, what if they are not respected? What if they are not respected? What do you mean that they are not respected? As I said earlier, some pairwise comparisons in our solution may get violated, so let me explain that in the next slide. Instead of writing the expressions here, let me explain that here. So, let us say that we already know these. We already know these locations Yjp is known. For a set of ordered pair omega. We need to find W and X such that weights are non-negative. That is what we said Wp has to be greater than or equal to 0. This means Wp is greater than or equal to 0 for all p's, at all p's. Secondly, the distance equals, the distance inequality has to be violated as less as possible. I want to respect this. I want to respect this. If I wish, if my solution does violate this pairwise comparison, I want that violation to be minimum which means I am allowing for this. I am allowing for some of these violations to happen, and I am allowing for some violations to happen. So, the objective function can now be formulated as a minimization problem, where we define the objective function as a poorness of it, not the goodness of it. We will define this as poorness of it. How do we define its poorness of it? Let us define the poorness of it as j minus k plus. What does this mean as j minus, what is generally X plus, what is the notation? It is a max of 0 and X, max of 0 and X. That is what it is. So, if X is negative, if X, let us say, is negative 2 then X plus will be 0. However, if X is positive 2 then this guy will be 2,. So, depending on what the value of X takes, the max function will change either it will be 0 or X. So, what will be this Sj minus Sk positive? Sj minus Sk positive will be defined as a max of 0 and Sj minus Sk. Now, what do we know? We know that the consumer prefers, we know that the consumer prefers j over k, that is, the ordered pair. We know that the consumer prefers j over k, which means we want Sj to be smaller, Sk to be bigger, we want Sj to be smaller, Sk to be bigger. Therefore, we want Sj minus Sk, to be negative. Why do we want it to be negative? Because of this, we know that Sj has to be less than or equal to Sk, Sj has to be less than or equal to Sk. Therefore we want. Therefore we want this Sj minus Sk to be negative. Now, you know that if X is negative 2, X is negative 2, then the max value should be 0, the max value should be 0. However, let us say that this is violated. This distance inequality is violated, which essentially means that even though j is preferred over k, even though j is preferred over k, somehow Sj is bigger than Sk. Now, that is a violation, that is a violation. Let me be very clear on this. Even though j is preferred over k, we are saying that Sj is bigger than Sk, and therefore this max function turns out to be Sj minus Sk. If that happens, if Sj minus Sk is actually greater than 0, which is why the max function will return Sj minus Sk. If this happens, then it is a violation of the distance inequality, and therefore it represents poorness of it, it represents poorness of it. Let me say that again if the distance inequality is not violated ideally, the value of this guy. Ideally, the value of this guy should be 0. Ideally, it should be 0 because if this is negative, and if the distance inequality is not violated, Sj minus Sk will be negative, and therefore the max function will be 0. Ideally, this value should be 0. Only when the distance inequality is violated, that is when Sj minus Sk will take on a value greater than 0, and therefore this will be a positive value. This will be a 2 here because this X is positive, this is positive, and therefore this will not be 0, and that will represent its poorness of it. So, I hope this explanation of its poorness of it is understood. But what we wish to do is? We wish to minimize this poorness of it. We wish to minimize the distance inequality violations. We wish to minimize these distance inequality violations. So, that is our objective function, that is our objective function. Now, if you refer to the article, it gets into the details of how do you actually achieve this objective? It actually defines something called the goodness of it. Just like there is a poorness in it, there is goodness in it. And then it says that the difference between G and B, the difference between the goodness of it and the badness of it, or the poorness of it should be some constant value h. It gets into those technicalities. We are not getting into the technicalities. We wish to quickly get into the formulation of the problem as a linear programming problem. So, let us define ajkp. ajkp as a parameter which is the square distance squared Ykp minus Yjp square, Ykp square minus Yjp square, once again Ykp is known. Therefore Yjp is also known, and therefore, this is calculatable, and therefore this is a parameter. Similarly, bjkp as negative 2 into Ykp minus Yjp, for example, for our problem, let us say for a pair 1, 3 for 1, 3 attribute 1, how will you calculate this? This will be, so this is your jk and p. So, j is actually equal to 1, k is actually equal to 3, p is actually equal to 1. So, a value will have to be calculated for each attribute. How will you calculate this? You will calculate this as y31 squared minus y11 square. That is how you will calculate this. k is 3, attribute 1, k p square, j, j is 1, p is 1, so y11 square. So, go back to the problem where is my data, where is my data, way back all here. So, y third option, okay, attribute number 1, attribute number 1, so Y31 is 2.3, Y11 is actually 1.5, Y11 is 1.5 that is how you define these Y's. We already knew this. But we use that now in the calculation for ajkp. So, this was my 2.3 squared minus 1.5 squared. I do not, and whatever that number was, and that is how you will calculate ajkp for each attribute, for each pair, for each pair jk, you will calculate this. Similarly, you will calculate bjkp. So, just like there is a31, there is going to be a132 for the second attribute, and there is going to be a b131 and b132. Attribute 1 for pair 1, 3 attributes 2 for pair 1, 3 b value. Once again, notice that this is also a parameter because y's are known, and therefore B will be known as these are parameters. Now, let us define a new variable called Vp, which is Wp into Xp, p and Xp were our original decision variables. Let us multiply them and define a new variable called Vp because we want to make the problem linear. Let us define a parameter called Zjk, which is just a convenience to get the problem in a linearized format. So, there is no geometric interpretation for Zjk. And let us add up all the a values, let us add up all the a values, let us add up all the a values for all the pairs, for all the pairs, let us add up all the a values for the second attribute, let us add up all the a values for the b values for the attribute. Let us add up B values for the other attribute and define what is essentially Ap and Dp. Now, Ap is the summation of all the attributes. So, you will get a1 and a2, which is a summation over all the pairs, summation over all the pairs. Similarly, you will get d1 and d2, which is the summation of all the bj's. Keeping the notation same, these are all the notations that were used in the problem formulation originally in that article. Once you do that, these are all technicalities. We need not get into how we get here. If you want to know how we got here, I strongly recommend that you refer to that original article, which is just an awesome seminal article written way back in 1973. Once you get the parameters and remember that these are parameters, the values are known Ap can be calculated. Once you know the coordinates and the pairwise preferences, Dp can be calculated. Once you know the attribute values and the pairwise references, so Ap and Dp are parameters. They are not variables. I know them apriori. What I do not know, what I do not know are my new variables W and V, W are the weights. Now, V is the new variable which I have defined using multiplication of my two original variables. And now, the formulation can be written as minimization of summation Zjk, summation of Zjk subject to this constraint which is wpajkp, vpbjkp and Zjk. And the second constraint is the normalization constraint, where WpAp plus VpDp is actually equal to 1. And what are my decision variables? My decision variables are Wp, Vp, and actually, Zjk, Zjk, and notice that in terms of these three decision variables, this formulation is actually a linear programming problem. Look at the objective function. The objective function is simply a summation of the Zjk values, so it is a linear function of Zjk. It is a linear function. Now, the constraints, constraints are a linear function of Wp, Vp, and Zjk. It is a linear function, linear function. Similarly, the normalization Wp, Vp are in the linear relationship in a linear relationship, so even this constraint is linear, this constraint is linear, the objective function is linear, and therefore this problem is a linear programming problem. Now, this problem is going to give you the optimal values of Zjk. It is going to give you the optimal value of Vp. It is going to give you the optimal value of Vp. Now, Vp you anyway wanted, Vp you wanted because these are the optimal weights assigned by this consumer to each of the attributes. Remember, you are going to get w1, w2, w3, all the way for each attribute. Zjk, you can throw away, you do not need them,, you do not need them. Now, what do you? What did you want? You also wanted the location of the ideal point. How do you find the location of the ideal point? That was X. Now, how where is X? You did not calculate X. You only calculated V, but from V, if you notice, you can calculate X. So, how will you calculate X? X is going to be calculated as W divided by V. So, Xp is calculated as Wp divided by Vp, and that is how you are going to get the coordinates of your ideal point. So, you will get your Wp, which is the weight assigned by this consumer for each attribute. Vp is going to give you X. How? It is Wp divided by Vp, sorry reverse, reverse, sorry, Vp divided by Wp, did I say that here,, sorry, here I said it incorrect here also. So, it is Vp divided by Wp because what is V? V is W into X, so what is X? X is V divided by W, so this is incorrect. This is correct. So, this is incorrect. Vp divided by Wp. Now, Vp is obtained from the optimal solution of this LP. The optimal solution of LP is going to be with the values of Vp. Wp you have anyway calculated, from there you can calculate Xp as a ratio of Vp divided by Wp. Now, you have your optimal weights, you have the coordinates of your ideal point, and then the problem is solved. That is what you wanted originally. So, this is a linear programming approach to get the importance given by this consumer to each of the attributes and the ideal product that would be preferred by this consumer, solving this as a linear programming problem. So, this was only a formulation, we will stop here, and next session, we will actually look at a numerical example, where we will solve this linear programming problem, get the solution, and interpret that solution in terms of our attributes and choices. Thank you.