In the last video, we saw how to, how to get the predicted value of Y, we are going to compare the predicted value of Y with the observed value of Y. And we are going to look at how good my model is by looking at this comparison. If a lot of times my prediction has been right, then I would end up concluding that my model is good. On the other hand, if a lot of my predictions are off, I have predicted that the student will not get the job and vice versa, then obviously, my model is not good enough. So, how do you evaluate a logistic regression model is the next obvious question to answer. Once again recalling your memory. So, what we are doing now, we are essentially comparing the observed value of Y with the predicted value of Y with the predicted value of Y. And we are asking ourselves, what is, what is a good way to say that my logistic regression model is good. My logistic regression model is good. So, there are very robust statistical methods that are used to calculate or evaluate the goodness of a model. And, how do we say that the logistic regression model is good and good and good? So, as I said, there are statistical methods, those are generally based on the Log-likelihood, the value of the Log-likelihood. And you scan any statistics textbook and you will observe these statistical indicators deviance, which is I think negative 2 into LL, LL is Log-likelihood, negative 2 into LL there are some aspect criteria, even R squared is dependent on Log-likelihood, I think there is a square root formula, which is again dependent on the value of the Log-likelihood again negative 2 into Log-likelihood. There are some information criteria like Akaike information criteria, Bayes information criteria. Now, these statistical, statistical indicators obviously, cannot be taken and in absolute sense. So, whether the value of the statistical indicator is large value or small value, maybe again dependent on some statistical test. For example, some of these statistics have a chi squared test statistic, the Akaike information criteria, the Bayes information criteria, whether they are large or not significant or dependent on some chi squared test. Or the other indicators, maybe higher the better type, for example, R squared, R squared, we never have any statistical test, R squared we say larger the better. In that, that is what we saw in our MLR also, what was the criteria for R squared to be acceptable as large as possible. So, some of the statistical indicators may be of higher quality also. And the example of higher the better type is R squared. The Akaike information criteria, Bayes information criterion they have chi squared test statistic. And once again, you know how to run those chi square test statistics. There is going to be a hypothesis that the logistic regression is significant or the logistic regression is not significant. And you calculate the test statistic which is a chi square test statistic, you compare that chi square test statistic with a tabular value of the chi square test statistic. And if the calculated value is more than the tabulated value, then you reject the null hypothesis and conclude that the logistic regression is significant. So, that kind of thing, we are not going to go in that direction, we are not going to build a statistical test chi squared based test, let us do that later when we see that in Python. For now, when we are evaluating everything in excel, let us define some statistical indicators which are higher the better type, but we are not calculating R square either. So, what are we going to calculate? We are going to calculate three very commonly used statistical indicators. Those are called accuracy, precision and recall. Obviously, this should tell us whether our logistic regression model is good or not. If my accuracy score is fairly large, I should be able to say; Yeah, my logistic regression model is good, if my precision value calculated turns out to be fairly large, logistic regression is good, same is true for recall. So, we are going to calculate these three performance indicators for our example. Before we calculate these performance indicators, let us define these performance indicators. So, let us define accuracy, precision and recall in the context of our logistic regression model. And as we can clearly see, they are larger and the better type. What is the accuracy I want for my model? As large as possible. What is the recall I want for my model? Obviously, larger the better, higher the better, larger the better whatever you call it. So, let us define these accuracy, precision and recall. Accuracy, how do we define accuracy? It is the measure of total predictions that a model got right. This includes both true positives and true negatives. So, accuracy is just an overall measure. Tell me how many times you got it right? If you said Y will be 1 was Y 1? If you said Y is equal to 0 was Y 0? So obviously, I am interested in the true positives as well as true negatives, I am interested in both. Y is equal to 1 matching in observation and in prediction. Y is equal to 0 matching in observation and prediction. So, tell me the total number of correct predictions for your model. How many times did the model get it right? That is accuracy. Second evaluation criteria is recall. Recall is the percentage of the response values that were captured by the model. What was the response value? Response value Y is equal to 1, the percentage of the response value that was captured. So, there was a certain number of Y that was equal to 1 in the observation. How many times, how many times did the model match with your observation, when the observation was Y is equal to 1? That is recall. And lastly, precision, precision is the measure of predicted values that were correct. When the model predicted Y is equal to 1, was Y 1? This is exactly opposite of what recall was. We will, we will discuss that in the next slide we have that. So precision, again, is a measure of percentage of the predicted response values being correct. Response values that we are interested in. So, we will only take Y as equal to 1. We will not take Y as equal to 0. So, for our student placement data, how do these evaluation criteria translate? So, for our response for our student placement, example, our response variable was binary. And we were interested in one particular value, which is a student getting placed, so we were interested in Y being equal to 1. So, how do we define accuracy? We define accuracy as a ratio of the total number of times the predicted and actual value matched both for Y is equal to 0 and Y is equal to 1 to the total observations. What were the total observations in the sample? 27, because we had data for 27 students, out of the 27 observations in the sample, how many times did the predicted and actual values matched both for Y is equal to 0 and Y is equal to 1, that ratio is going to be defined as accuracy. Recall, recall was how many times did the model correctly predict when the observation Y is equal to 1? So, the total number of times that the prediction was Y is equal to 1 to the total number of instances that the sample values where Y is equal to 1, the observations where Y is equal to 1. So here the total, the total set is when the Y was equal to 1 and how many times was my model able to capture that. Precision. If you want to take a minute to digest this, let us read it one more time. It is the ratio of the number of times the prediction for Y was 1 to the total number of instances where the Y was 1. Precision, on the other hand, is exactly opposite. It is the ratio of the number of times the Y was 1 to the total number of instances where Y was predicted to be 1. Exactly opposite, exactly opposite, with these three definitions, let us evaluate our model. I hope you have understood these three evaluation criteria; accuracy, recall and precision. I want all these three to be as large as possible for me to conclude that my model is good. Makes sense, right? Accuracy, recall and precision. Towards that, let us go back to excel and calculate these performance indicators for our model. So, once again these are, these are our observed values of Y, and these are our predicted values of Y. Let us do this. So, predicted values can be 0 or 1, actual observed values can be 0 and 1. So, from the data, you can verify it is only 27 values. You can verify that when Y was 0, how many times did we predict that theY would be 0. 14 times, when Y was 0, 3 times we ended up predicting that theY would be equal to 1. So, this was wrong, this prediction was wrong. So, nearly this prediction was wrong, the actual value of Y was 0, our prediction turned out to be that Y will be, actual value of Y was 1 and the predicted value of Y was 0. We had two such instances. So, this 3, and this 2 are incorrect predictions out of the 27 observations. So, how do we calculate accuracy? What was our definition of accuracy? Our definition of accuracy was the total number of times our prediction was correct to the total number of observations. What is the total number of observations? Total number of observations is 27. How many times was the prediction right? 14, 14 times actual value of Y was 0, predicted value of Y was 0, good. 8 times, 8 times the predicted value of Y was 1, the actual value of Y was 1. So, 14 plus 8 matches. And therefore, accuracy is calculated as 14 plus eight divided by 27. What was precision? I calculated precision before recall, here I have defined recall before precision, does not matter. What was precision? Precision was the ratio of times the Y was actually 1 to the total number of instances where the predicted value was 1. Whereas the predicted value of Y, predicted value of Y was 1 when it was actually 1 is 8. And, what is the denominator? Denominator is the total number of instances where the prediction was 1. Total number of times the prediction was 1. What was the total number of times the prediction was 1? 11 times the prediction was 1, 11 times the production was 1 out of these 11 times, 8 times the actual value of Y was actually 1. So, 8 divided by 11 is my precision of the model, 11 times totally my, my model predicted that Y will be equal to 1 out of those 11 times, 8 times Y was actually 1. What was recalled? Recall, first of all, let us get the denominator. It is the total number of instances where the sample value actually said, Y is observed to be 1, Y was actually 1. So where am I? Where am I looking? I am looking at this value, 10 times in the data, actually i was at 1, 10 times in the data, y was actually 1. How many times was I able to recall that 8 times my prediction was Y will be equal to 1, actually 10 times Y was equal to 1 out of those 10, 8 times I correctly captured that. And therefore, my recall is 8 divided by 10. Once again, accuracy, recall and precision. They are the performance indicators for my logistic regression model. I want them to be larger, the better. I want them to be larger, the better. Now let us go back and check. How are these values influenced? These values are influenced by, how many times did I get it right? How many times did I get it right? And how many times did I miss it? So, it depends on my predicted value of Y. Now, where are these predicted values of Y coming from? My predicted values of Y are coming from the probability of day 0, which is predicted by the model which is calculated by the model and more importantly by the cut-off. Remember earlier we said this cut-off seems to be arbitrary. This cut-off seems to be arbitrary. Now, if I change the cut-off from 0.5, let us say 0.6. Any probability, which is lesser than 0.6 will be classified as Y is equal to 0. Any probability, which is more than 0.6 will be classified as Y is equal to 1. So, the predicted value of Y's depends on this cut-off. And therefore, the accuracy, precision and recall should also depend on this definition of my threshold, this definition of my threshold, whether you call it cut-off, you call it threshold, it is the same thing, it is the same thing. So, if I change the cut-off, from 0.4 to let us say from 0.5 to let us say 0.4, the accuracy values should change, accuracy value should change. Can I see that? Can I see how the accuracy, precision and recall change? If I change this cut-off, then I should be able to correctly set the value of the threshold, which gives me the maximum possible value of accuracy and precision recall. That would be the correct approach. Well, that is what we have done. So, these are the various values of cut-off, cut-off could be 0, 0.1, 0.2, 0.3, 0.4, 0.5, all the way to 0.9. And these are the various values of accuracy, precision and recall. Notice that precision and recall, they were exactly opposite of each other, and therefore they are moving in the opposite direction, they are moving in the opposite direction, when precision is quite large, the recall is quite low, when the recall is quite large, precision seems to be quite low, expected, because that is how we define them. Alright, so what do we need to do? What do we need to do? We need to change the cut-off to let us say, 0.1, 0.1. Calculate all these values. Calculate all these values. I am not set up, yeah, it is set up correctly, is set up correctly. So when, when the cut-off is 0.1, when the cut-off is 0.1, the accuracy is 0.7778. Precision is 1, and recall is 0.647, 0.647. When the cut-off is, when the cut-off is 0.2. When the cut-off is 0.2, my accuracy goes up. Earlier, it was 0.77, now my accuracy is 0.81. My precision remains 1 but my recall goes up, my recall goes up, so 0.1 seems to be inferior, and 0.2 seems to be a better value. I will keep doing this, I will keep doing this. Keep going all the way to 0.9. Before I do that, let me set that 2.5 as my default threshold. That is how we saw the calculations. And let us look at the plot of this. Let us reduce the zoom a little bit so that I can fit that into the screen. So, as the cut-offs, I need to show the plots also. So as the cut-off increases, as the threshold probability increases, the blue curve is the accuracy curve. The orange curve is the precision curve. And the gray curve is the recall curve. So, looks to me, if I set the threshold to be too low, my precision will be quite large, but I will take a hit in recall. Whereas if I set the threshold to be too large, my precision will take a hit. But I will perform very well on recall. I will perform very well on recall. So sometime somewhere in this region, somewhere in this region, is where I seem to be doing well. Somewhere in this region is where I seem to be doing all right. So somewhere in this region, 0.5, 0.4, from there looks like 0.4, 0.4 seems to be a good threshold to be set, but that is the conclusion. Let us go back to evaluation. How did we evaluate our logistic regression model? We evaluated our logistic regression model on three parameters; accuracy, recall and precision. All three of them are higher, the better kind of criteria. I want all three values to be large. And therefore, I said that the threshold probability to classify Y is equal to 0 or Y is equal to 1 will be set based on getting the maximum possible value of accuracy, precision and recall and there is a trade-off. What is the trade off? We saw that precision and recall seems to be moving in the opposite direction. What is another quick way of checking whether my model is correct or not? This is a very analytical way of looking at whether my model is correct or not, this is, this, this should take precedence. The other way to look at whether my model is or wrong is to quickly look at the plots. So, I have not done anything different, all the data is exactly the same. Now, I have plotted. Probability of Y is equal to 1, predicted, predicted value of probability of Y is equal to 1 against the MBA CGPA, CGPA during the MBA program, let us zoom it in a little bit. Take a closer look at this, take a closer look at this. And clearly, we seem to be able to see that when the CGPA is low, when the MBA CGPA is low, the probability that the student will get placed is low. As the CGPA increases, there seems to be a sudden spike. Sudden spike and obviously, it is a probability, so it has to taper off at 1. So, this is that famous S curve, obviously, our S curve is not very precise. But this is what I would have expected my response variable to behave. Now back to, for MBA CGPA, my beta was positive, my beta was 3.27. And therefore, it resulted in this kind of a response behaviour when we looked at one of the explanatory variables, so, we can look at this and we will say that, well, yeah, the model seems to be behaving the way I expected the model to behave. But this is a more precise way, this is a more analytically correct method of deciding whether my model is good. So, looking at these numbers, even if I look at 0.4, I get the maximum accuracy of 0.85, and my precision and recall are 0.81 and 81 respectively, same values, this seems to be the best combination. And looking at these values, 85 percent accuracy, 81 percent precision, 81 percent recall, I would say, yeah, good, my model is not doing bad. And therefore, that is one way of concluding that what we have done in this excel is not such a bad thing to do. Once again, we are going to end the session here. And the last thing to be done is to appreciate our understanding of what these betas mean. Let us pause here for a moment and continue in a moment.