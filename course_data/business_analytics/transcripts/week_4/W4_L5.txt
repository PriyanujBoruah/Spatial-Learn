So, what does this problem looks like? Any guesses. To me, this problem looks like a simple linear regression problem. So, a Simple Linear Regression, SLR, Simple Linear Regression, may help us calculate the y-intercept. Which are the market size and the slope. So, SLR will tell us if the linear relationship is a good fit for the data available from the market experiment. Now, what do we do when we are thinking about a non-linear relationship where we are, we want to keep the elasticity constant, what do we do with that? Can we use SLR in those cases? What is the guess? Can we use SLR in those cases? What is the answer? Probably, yes. And we will see how that can, how SLR can even help in that case. So, what I have is sample data and let us build a corresponding simple linear relationship, a simple linear regression model for that data. So, let me stop the presentation and go to the Excel sheet. So, let us say, let me increase the zoom here. So, let us say that these are the various prices offered and these were the corresponding demand at those particular prices, various demands at these particular prices. When the price was 9 rupees, the demand realized was 2,891, when the demand was 32, the price, when the price was 32, the demand was 370, when the price was 26, the demand was 2,946. So, these are the different combinations of price and demand at that particular price. So, let us see if the linear relationship fits better. So, what are we going to do? Let us understand the plot. So, what I am going to do is draw a scatterplot. So, the x-axis is the price, the y-axis is the demand. This is a typical demand response curve. So, these are the realized points. So, when the price was 5 rupees, the realized demand was 6,707. When the price was 39, the realized demand was 297. When the price was 35, the realized demand was 484. When the price was 40, the realized demand was 193. So, now let me try and fit a linear relationship between price and demand. Let us try to fit that. So, what I will do? I will go, add a trend line. I will add a linear trend line. And I will ask Excel to print the equation and the r square for the relationship. Do we know how to interpret this equation? And do we know how to interpret this r square? Do you recall your DM course, probably it was discussed there? Anyway, does not matter. Let us read this value. So, what is this? This is, what is the equation? The equation is y is equal to 5842.8 minus 157.7 into x. What is y? Y is demand. What is on the y-axis? Y-axis has demand. Demand is 5843. Let me write, let me round this up to point 8, I will write this as 5843 minus, I will round this up and I will say 158 into the price because the x-axis is the price. So, how do you interpret this 5843? Compare that with a linear relationship which is D is equal to D0 minus m into p, so this is your D0. This is the demand when the price is equal to 0. Even if you offer the product for free, what is going to be the demand? The demand is going to be 5,800 odd. Now, for a unit change in price, per unit change in price how much do you expect the demand to change, that is given by the slope. So, for a unit change in demand, for a unit change in price, the demand is going to go down by 158 units, a negative slope. You increase the price by 1 unit, the demand is going to go down by 158 units. That is what the predicted equation says, prediction equation says. Now, what is this R square of 0.733? What does that mean? Do you recall? For that let us run a simple linear regression model. How do you run a simple linear regression model in Excel? So, I go to data. I have a chance to show you the Excel add-in. There is no data analysis toolpak here. So, let me add a data analysis toolpak. So, we will go to options, go to add-ins and I will ask Excel to add analysis toolpak, data analysis toolpak. I will say go. So, I will add an analysis toolpak. Let me not add solver add-in right now. Let us add that. Now, you see one more option available here called data analysis. Let me use that data analysis. Let me go to the option called regression. What are my y values? My y values, my response variable, my dependent variable is demand. So, I will say this is my variable. What is the explanatory variable, x values, the x-axis is the price? So, these are the various price values. I will tell Excel, this is where the prices are. Labels, yes, I have labels in the first row, yes. Output range, let me ask Excel to print the output here and let us print it. So, this is how Excel runs regression. Now, let us understand, let us interpret these values. So, this 0.85, is multiple R. What is multiple R? If you recall, Excel had an option called regression. What we are running is what is called simple linear regression. Excel does not differentiate between simple linear regression and multiple linear regression. So, Excel simply says regression. Therefore, the R that is run, the R that is calculated is called multiple R, more suited for MLR, Multiple Linear Regression, but this R represents the correlation coefficient between the price and the demand. Do you recall what the correlation coefficient is? You must recall it from the DM course. So, how do you, independently how would you calculate a correlation coefficient? So, there is a function in Excel called CORREL, correlation, and you say that these are, this is my array number one, you do not have to have the label, and this is my array number two, it is negative 0.85. Negative relationship, because as the prices are going up, the demand is coming down. So, this is the correlation coefficient between prices and demand. Now, it should not matter whether I put the demand array first or the pricing, the prices array first. So, here I have put the demand array first and the prices array later, does not matter. Correlation is, does not depend on which variable is entered first. So, even if I entered this first, this later, the value should not change. So, it does not matter. It is just the relationship, it is just the correlation coefficient between two arrays of data. So, what does this 0.855 represent? 0.855 represent a correlation coefficient between price and demand. The negative value represents the sign of correlation. And the value 0.85 represents the strength of correlation. The value is closer to 1 stronger than the association between the variables and the negative or positive value only tells us the direction of the association. As the prices go up the demand goes down, and therefore, that is represented by this negative value. Now, what is the R square? R square is simply the square of the R. If I square this correlation coefficient, if I do this, this square, I am going to get R square. R square is the square of the correlation coefficient. Now, there is one more interpretation of the R square. Let us do that little later. Now, what is that adjusted R square? Let us leave the adjusted R square discussion also to a case when we are discussing multiple linear regression. Adjusted R square only makes sense when we have more than one explanatory variable. So, let us go back to PPT. Now, I have explained the important things. I have explained the prediction equation. How do I predict demand? So, I have predicted the y-intercept which is 5843, the predicted value of the slope is, the estimated value of the slope is 158, and we have explained R square which is the square of the correlation term. Let me go back to PPT and explain a few more details about the simple linear regression. So, what is simple linear regression? Simple linear regression describes how the conditional mean of Y changes with different values of X. So, what is conditional mean? What do you mean by conditional mean? You give me a value of X, I will tell you what is the value of Y. what is the value of Y? Value of Y is going to be beta naught plus beta 1 x. What is this beta naught? Beta naught is D0 for us. What is beta 1? Beta 1 is m for us. So, this is called the conditional mean of y with respect to x. So, the simple linear regression model, simple regression model, simple linear regression model essentially tells us that it is going to be a line with an intercept of B0 and a slope of B1. The intercept was the y-intercept which is D0, which is the market size. This is, however, the expected value of Y. This is the expected value of Y. So, however, the points that we saw, the points may, the observed points may be away from them, away from this line. So, the gap at any price point this gap between what is the expected value of Y and what is the actual value of Y this gap is essentially the error in the regression model. So, the deviation from the mean is called error. Errors are usually represented by epsilon. I should not have confused that with elasticity. But let us also denote error by epsilon and this is not to be confused with elasticity. Elasticity is also epsilon, the error is also epsilon, but they have no correlation, only the same notation. So, on average, we do not expect any errors. Therefore, the error terms are expected to have a value of 0 on average. So, what do I mean by, once again, what is this deviation? So, this deviation is, this is the prediction line. Where is this prediction line coming from? This prediction line is coming from the data that we have collected. So, this may be data and this is the production line. So, what are we saying? We are saying that at this particular price, let us say P1, the x-axis is always price, the y-axis is always demand. This is my predicted value of y. And how is that given? That is calculated using this expression, beta naught plus beta 1 x. That will tell me what is the expected value of demand at that particular price. However, the realized value is somewhere here, the realized value was somewhere here. So, this gap, the vertical gap between the actual value of demand realized at that particular price point and the predicted value of y coming from the simple linear regression this gap is called error. Now, on average, I do not expect this error. Therefore, on average, the expected value of error is 0. So, what is, what are errors? Errors are deviations of responses from the predicted value of y. Where is the predicted value of y? This is the predicted value of y. This is the actual value of y. At this particular price point, let us say P3, the predicted value is here, the actual value is here, so this is the y, this is the error. So, now then how am I supposed to draw this line. I am supposed to draw this line in such a way that these errors are minimized, but you realize the problem with that. Sometimes the error may be positive. Let us say if I calculate this epsilon as y minus y hat, y is the predicted value of y, let us say, and y hat is the observed value. So, the predicted value comes from this beta naught plus beta 1 x minus y hat, y hat is the actual observed value. Now, sometimes y minus y hat may be positive, which is in this case, y minus y hat turns out to be positive, sometimes the y hat may, sometimes the epsilon may turn out to be negative, y minus y hat may be negative. Now, if I say that, let us say this is my error one and this is my error three and this is my error seven at different values of price, so if I simply add up the errors, e1 plus e2 plus e3 all the way to en, I have n values in my sample. And if I say that minimizing this, it may not work out because some of the positive errors may cancel out some of the negative errors. And I may in general underestimate the total error in the model. Therefore, even though the objective is to minimize the errors, we usually do not minimize the sum of errors. So, what do we do? What is the method of nullifying the effect of positive or negative error? Usually, we square the error terms, we usually square the error terms, and only then minimize the summation. So, in general, what are we minimizing? We are minimizing the sum of square errors, sum because it is all summed up, the sum of square errors. That is the objective with which we run the regression model. So, what is this? This is a mean square error line. This is the line where the mean sum of square errors is minimized. That is how we generate the line. What are the properties of this error term? We are making essentially three assumptions about the error term. We want the error term to be independent. What do I mean by that? We do not want the error term even to be dependent on error term e2, we do not want error three to be dependent on e1 and e2, we do not, in general, want these error terms to be independent. We want these error terms to have equal variances. The variances of all the error terms should be sigma epsilon square. And we want these error terms to be normally distributed. We want the error terms to be normally distributed. These are the three assumptions about the error terms. Now, even before we interpret our simple linear regression model, we should confirm that these assumptions hold. If the error terms are not independent, if the error terms do not have equal variances, if the error terms do not have normality, we are going to say that the assumptions are violated, and therefore, we should be very, very careful in interpreting the results of our regression model. How do we check the assumptions? We will do that little later. Right now, let us start by saying that we assume that these three hold. We assume that the errors are independent. We assume that the errors have equal variances. We assume that the error terms have a normal distribution. But these assumptions have to be checked. So, with these error terms, what is the observed value of y? The observed value of y is the predicted value of y plus the error terms, where error terms have a normal distribution. Why do I say that the error terms of normal distribution, because that is my assumption? On average this is the mean of the normal distribution, this is the variance of the normal distribution. On average, we had anyway said that the expected value of error terms is 0. On average, we expect the error terms to have 0 value. So, the mean is 0 and the variance is sigma epsilon square. So, that is how we write this. It is a normally distributed error term with a mean 0 and variance sigma epsilon square. Once again, because the error terms have these three assumptions because we want the error terms to have these three properties, we are going to now say that the observations are independent of each other, the observations y are independent of each other, they have equal variances around the regression line and they are normally distributed around the regression line. What do I mean by that? So, essentially, if we say, these are the value of x, this is the value of y, and let us say that this is a regression line, our regression line was downward sloping. So, let me draw this as a downward sloping line. Now, at a particular x, at a particular value of x, let us say x1, this is the predicted value of y, this is the estimated value of y. This is what the regression line is going to give me beta naught plus beta 1 x. That is what the line is going to give me. The observation, however, could be anywhere. So, what is the, where is the observation? The observation we are going to say is normally distributed. It can be anywhere on this line. It could be, the observed value of y could be here, could be here, could be here, could be here, could be anywhere. The expected value is going to be here. Similarly, pick a different value of x, let us pick it sufficiently away so that I can draw this properly, let us pick a different value of x. This is going to be the point on the y, a point on the green curve is going to be the predicted value of y. So, this is the predicted value of y. However, where can the y, actual y be? Actual y, so the actual y could be anywhere. So, it could be here, it could be here, it could be here for that particular value of x. So, because of this error term, the observed value of y could be anywhere because of the error terms. So, we can keep drawing this normal distribution curve for each value of x. We can keep doing this. Let us stop and go back to the Excel sheet. Let me stop the presentation. So, what is the standard error now? We have seen this is essentially your sigma epsilon, the standard deviation of the error term. This is not a square value, this is a standard error. So, this is the standard deviation, not the squared value. So, now, we said we are going to defer the interpretation of 0.724 later. When we take up an example of MLR we had described this, we had described this, now we are describing this, which is the standard deviation of the error terms. A number of observations, there are 31 observations. You can notice there are 31 observations. The first row is the label. So, there are 31 observations. So, n is 31. So, this is that n 31. So, how do you interpret this ANOVA table, first of all, degrees of freedom? Why do we have one degree of freedom for regression? Because what is the prediction equation. The prediction equation is y is equal to beta naught plus beta 1 x. So, how many values are we estimating? We are estimating the value of beta naught, which is the y-intercept, we are estimating the value of slope beta 1. So, we are estimating two parameters. So, degrees of freedom will be 2 minus 1, this is 2 minus 1. Why is the total degree of freedom 30? If you recall, remember your regression discussion in the previous course, why is this 30? This is always n minus 1. I had 31 observations, 31 minus 1 is 30. Why do I get 29 here? So, the total degree of freedom is 30. So, this is 30 total degree of freedom minus the regression degree of freedom is 1, so 30 minus 1 is 29. That is how I get my degrees of freedom. Why do I get 1 here? Recall your discussion of regression from the previous courses. It is two parameters being estimated beta naught and beta 1, 2 minus 1 is 1. Why is this 30? Total observations are 32, sorry, 31, total observations are 31, 31 minus 1 is 30. Why is this 29? Total degrees of freedom is 30, regression degrees of freedom is 1, so 30 minus 1 is 29. Now, the sum of squares, I am not going to get into details of the sum of squares. I am going to rely on your previous course. Otherwise, I will put up a primer on how do you interpret the sum of squares for regression and the sum of squares for residual. The Sum of squares for residual and sum of squares for regression essentially comes from how do you calculate the sum of squares for regression and the sum of squares for error. So, what is this value? What is this large value? This large value is your SSE, the sum of squares of the errors, residuals are also errors. So, this is your SSE. This is called SSR or SSM, the sum of squares of the model or the sum of squares of the regression. Usually, if you call it SSR, you have to be careful because is it SS for regression or is it SS for residuals you have to be careful. So, you can always call this SSM, the sum of squares for the model, sum of squares for error to avoid any confusion. How is mean square calculated? Mean square is calculated as the sum of squares divided by degrees of freedom. So, this 133 is calculated as the sum of squares divided by 1. How is this mean square error calculated? The mean square of error is calculated as the sum of squares of the error divided by degrees of freedom. So, if you want to verify this, how will you get this? So, this is calculated as the sum of squares divided by degrees of freedom and that is how you get your mean sum of the square. Now, Mean Sum of Squares, this is Mean Sum of Squares, this is still the sum of squares. So, this is MSE, Mean Sum of Squares, but this is still the sum of squares. Now, if you take a square root of this, what are you supposed to get? You will get your standard error, because we just standard deviation, you take a square root. The square root of MSE will give you the standard error. So, it is that value. So, let us get rid of these. We do not want this. We only want to interpret the Excel output. How do you calculate the F test statistic? First of all, what is the F test statistic? This is a test statistic. So, this is a test statistic. What is the test statistic? The test statistic is usually for a hypothesis test. So, what hypothesis are we testing? We are testing a hypothesis that the regression model is significant or not significant, H naught and H1. So, here we are saying the regression model is not significant. The null hypothesis is regression model is not significant. Now, what is the regression model? The regression model is beta naught plus beta 1 x. When will the regression model not be significant? When will we say that x has no bearing on the values of y? When will we say that? When this beta 1 value is 0, if this beta 1 value is 0, then x will have no impact on y and then your entire regression model will collapse. So, what is the other way of saying that the regression model is not significant? You can say that this beta 1 is 0. Once beta 1 is 0, the regression model will not be significant against the alternate hypothesis that regression is significant. So, the overall significance of the regression is tested using this F test statistic. Now, let us not get into the distribution. It has an F distribution which has two degrees of freedom, one is numerator degrees of freedom, one is denominator degrees of freedom. Let us not look at the shape of the F distribution. Let us say that this is the test statistic value. Now, you usually look at the test statistic value and decide about the rejection of the null hypothesis by looking at the P-value. This is the P-value. So, this is the P-value for this hypothesis test. Now, how do you decide about the hypothesis, you compare the P-value with the significant value which is alpha, alpha is usually 5 percent, so 0.05, and if P is less than alpha, you are generally going to reject the null hypothesis. What is the P-value here? P-value is, of the order of 10 to the power of negative 10 which is anyway going to be lesser than 0.05. Therefore, we are going to reject this null hypothesis. What was the null hypothesis? The null hypothesis was that the regression model is not significant. So, we are going to reject this null hypothesis. And therefore, conclude that the regression model is significant. So, we say that the alternate hypothesis is good, a regression model is significant. So, that is the overall significance of the regression model is tested using this P-value. This is the overall significance test of a regression model. P-value is 7.8, 10 to the power of negative 10 anything of that order is going to be lesser than 0.05 and therefore we can safely reject the null hypothesis. So, that is how you interpret the upper block, ANOVA block of the regression output of excel. Take a minute to digest it. Let us hope that you have understood. If you have not, you can playback the video and go back, more importantly, go back to your regression discussion in the previous courses that should help you understand and interpret this ANOVA table of the Excel regression output. That is out of the way and we have confirmed that the regression is significant, let us interpret, let us see what is the estimated value of the y-intercept and the slope. So, y-intercept was 5842, we knew that 5842, 5842.8 was the estimate of the y-intercept, which is the total market size. What was the slope estimate? The slope estimate was negative 157, we knew that from the excel output earlier. So, these are your coefficients. So, what are we saying? We are saying that the estimate of beta naught, we cannot know the value of beta naught, we are saying that the estimate of beta naught, let us call it beta, b0, let us say that that is b0, that value is 5842, an estimate of beta 1, which is b1, what value is negative 157.7. And anyway, this is an estimation. This is the population value, this is the sample value, sample value from the 31 observations that you have. This is the sample value. This is how you are going to estimate the population value from the sample value, so these are sample values. So, there is always going to be some error. So, this is the standard error for estimating of intercept, this is a standard error for estimating the slope. Now, we can individually test whether this beta naught is 0 or beta 1 is 0 by running a localized hypothesis test. This was the overall hypothesis test whether the regression was significant, now let us run a local hypothesis test. What is this test statistic for? This is the test statistic for a hypothesis. What is the null hypothesis? We are checking whether beta 1 is 0, alternate hypothesis beta 1 not 0. What is the P-value? P-value is anyway very, very small. And therefore, we reject this null hypothesis and say that beta 1 is not 0. What about this, what about this? This is essentially checking a hypothesis that beta naught is 0 against the alternate hypothesis that beta naught is not 0. What is the P-value? P-value is of the order of 10 to the power of negative 15. And therefore, we can safely reject even this null hypothesis, and therefore, confirm that beta naught is not 0. Now, can you see a similarity? Look at this hypothesis test and look at this hypothesis test, more particularly focusing on the P-value. P-value here was 7.8, 10 to the power of negative 10. P-value here was the same. Why did that happen? This is happening because we are running essentially a simple linear regression model. In a simple linear regression model, what is the overall significance of regression? Regression will not be significant if beta naught is 0. So, when we are checking the overall regression significance hypothesis, we are essentially checking whether beta naught is 0. Here we are directly checking whether beta naught is 0. So, even though the test statistic use was different, here it was a t statistic and here it was an F statistic, the overall test is the same. We are checking whether beta naught is 0 or not. If beta naught is 0, your regression collapses. And therefore, it is not very surprising that the P-value is the same. So, if you rejected the null hypothesis of overall regression insignificance and therefore concluded that the regression was significant, you are going to pretty much conclude the same thing here by saying that beta 1 is not 0. Excel also prints the confidence interval, 95 percent confidence interval of beta 1. It says that beta 1 is going to be anywhere from 193 negative to 121 negatives. And what is the expected value? The expected value is negative 157. Similarly, for beta naught, beta naught is expected to be anywhere from 5023 to 6662 and the expected value is 5842. That is how you interpret the y-intercept and the slope reported by Excel regression output. Remember that this is only an estimate. You never, you are going to know the actual population value beta naught and beta 1. What you have is some estimate b naught and b1. So, that is all you can get from regression because you only have 31 values, 31-odd values, 31 exactly values. Let me conclude this regression output by telling you one more way of interpreting this R square. Earlier what did I, what did we say, R square was just the square of the correlation coefficient. Now, you can think of R square as the ability of the regression to explain the variability in y. So, one interpretation of R square is that the prices, these prices, different prices are helping me explain the variability of y. There is variability in the demand values. So, this regression model, what are, how can we interpret this 0.73, we can say that using price as an explanatory variable, how much explanatory power does it have, we can say that it can explain 73 percent variability in y. Let me say that again. Using prices as an explanatory variable can explain 73 percent variability in y. That is the interpretation of this R square. This is called the coefficient of determination. This is only a recall. This must have been done in the previous courses. We are only going to recall. How do you calculate this 0.733? So, what is the sum of the square, because of regression it is this value. What is the total sum of squares? It is this value. So, if I simply do this, this is the amount explained by the regression and this is the total variability that should be 0.733. So, because of regression, I can explain this much amount of variation out of the total variation in y. So, the R square is called the coefficient of determination. It tells me the amount of variability explained because of the regression model. Here, it is a simple regression model. I have only used price as an explanatory variable. So, what does that do to our demand response curve? Now, going back to the PPT, let me go back to PPT. So, here I estimated a linear demand response curve. I estimated the y-intercept. I estimated the slope. And therefore, I am going to fit a linear demand response relationship to my data. And what was the data? This was the data. And that is how we have used simple linear regression to predict our market size. Market size is expected to be 5842, total demand is supposed to be 5842. Even if you give away the product for free, we expect about 5842 demand value to be realized. Every increase in the prices, every unit increase in the prices is going to reduce the demand by 157 units. So, that is how we estimate the demand response curve. And let us pause here and end this session with an explanation of SLR applied to the price demand relationship. So, let us pause here, continue with this in the next session. So, I am pausing the video now.