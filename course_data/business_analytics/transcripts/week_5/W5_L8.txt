Hi everyone. Welcome to this lecture on implementing linear regression in python. I am Srivatsa Srinivas, a co-instructor at the IIT Madras online B.Sc Degree Program. We are going to look at a specific data set which has a price demand relationship and in that it has a constant elasticity. So, before loading the dataset, we import Pandas and Numpy packages, then upload ‘Constant-elasticity’ file onto google colab as follows. And then read-in the file with the sheet name in the file being ‘Even better’. The header of the file looks something like this. We have the following columns, but the variables of interest to us are the ‘Price’ and the ‘Demand’ and therefore we create a reduced dataframe df2. The first step in any simple linear regression is to look at the scatter plot between the independent and the dependent variable and see how it looks like. For that we import seaborn package and then run the command sns dot regplot between the independent and the dependent variable to see how it looks like. And it looks something like this. It is not exactly linear and if you would recall we can actually perform a transformation on this. As you can see here it looks something like this. So, what you see is something like this pattern and we can do a log of y and a log of x transformation on this. So, that is what we will do in the python implementation. But, before we do that we would like to look at the relationship between the price and demand itself. For that we used dot corr to calculate the correlation and then square that term to obtain the r squared value. And the r squared is about 0.7009. This is how price and demand are directly related. Now, we can then move to the transformation step of price and demand by using log transformation from the Numpy package. And we construct two new columns natural log of price and natural log of demand in dataframe two and it looks something like this. So, there are two ways of implementing a linear regression in Python. You could either use the ‘statsmodels’ or you could use ‘sklearn’. We will first look at how we could use statsmodels to implement linear regression in Python. And then subsequently look at sklearn. For that we import these packages and then we do our sns dot regplot to see how this scatter plot between natural log of price and natural log of demand looks like. If we change this to true, what we will obtain, is the regression line corresponding to this. And this is how it looks again. Now, in order to implement linear regression using statsmodels, we use smf dot ols meaning ordinary least squares regression. The formula being the dependent variable is the natural log of demand and the independent variable is natural logarithm of price. And the data is df2. So, we develop the model and store it in model 2 and this model 2 dot fit will construct the model. And if we look at the summary, we see that the r squared is about 0.819, much higher than 0.7009 and the p value is quite low meaning that the regression is significant. And you can also see that the natural log of price as an explanatory variable has a very low p value meaning that the null hypothesis of beta 1 being equal to 0 is rejected and therefore this variable is significant. The intercept value turns out to be about 11.23 and the natural log of price that is the coefficient of the linear regression is about minus 1.5061. And, if you want to verify the value of r squared here, you could very well do the correlation between natural log of price and natural log of demand and square it to see if they match. And as you can observe here they indeed tend to match. Apart from r squared there is another performance measure named RMSE which will help us look at how that model is performing. You would like to want a higher r square for a simple linear regression model. However, the RMSE or the root mean squared value has to be low for the model to be performing better. For that we need the predicted values of this regression that happens to be results2 dot fitted values and the observed values are already known with respect to the natural log of demand here. So, when we do observed minus predicted that is what the residual is. So, we calculate the residual values and the residual values turns out to be the following for each observation. The RMSE is nothing but the residual squared for every data point that is we calculate the squared error and sum it over all observations. That is why we have a sum term here for the list of all these data points and then divide it by n and then find the square root of it. That is what RMSE is. And we find that RMSE turns out to be, we have not run this cell yet, so when we do this RMSE is about 0.5877. So, recall the function for RMSE is this. So, we have the mean squared error here which happens to be yi minus yi hat. yi hat is the predicted value yi is the observed value. You square this divided by n then the sum it over all i's. This is the mean squared error and RMSE is the square root of this mean squared error. That is what we have calculated in the Python code. Now, we have calculated the values of linear regression using python code. We observed that the intercept is 11.23 and the natural log of price, the coefficient is minus 1.5. We will now verify the same by implementing linear regression using Sklearn. For that we import linear model from sklearn and mean squared error and r2 score from sklearn. And then the model is set up as follows. We have linear underscore model dot LinearRegression, which will help us set up the model. And we fit in two data sets here, which is the natural log of price and the natural log of demand. And, once we do that we have already constructed the model. As you can see here, the coefficient exactly matches with what we observed in statsmodels. It is minus 1.5 and the intercept also happens to be about 11.23 and it again exactly matches with statsmodels. The next two things are to check on the values of r squared and RMSE. For RMSE what we require is the predicted values of these natural logarithm of prices, based on the regression. For that we use natural log of price and use the regression model regModel2 to predict the value of the dependent variable for every value of the independent variable, natural logarithm of price. And then RMSE is calculated as follows. We find the square root of mean squared error function. The mean squared error function is between the natural log of demand and the predicted value of the natural logarithm of demand and when we do that, we obtain RMSE and the r squared value is calculated as r2score of natural logarithm of demand and the predicted value of the natural logarithm of demand. Once we do that what we obtain is an RMSE of 0.5878 which matches with what we had observed earlier and an r squared value of 0.8191. This is what we had observed earlier in this case. Now, going back to the linear regression model what we had done is we had done a log-log transformation. We had performed a log-log transformation on the given data set to obtain these values and we observe that the intercept value is about 11.23 and the slope is about minus 1.5, sorry, it should be minus 1.5. So, now when we go back and calculate the value of C it is going to be e power 11.23. Because, log of C is equal to 11.23 and the exponential of that will give us C and we have this epsilon meaning the elasticity to be equal to 1.5. So, if you recall how a constant elasticity model will look it was demand equal to C into the price power minus elasticity. And we have the value of C here and we have that value of elasticity here and we can go ahead and predict the demand as follows. This is what we had done in this tutorial. I hope you understand how to implement linear regression in Python. Thank you!