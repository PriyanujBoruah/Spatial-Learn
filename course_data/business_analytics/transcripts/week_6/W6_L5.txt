Let us take that other example. Let us take that other extreme example that I spoke about. I have another example I am not going to discuss this example a lot but anyway we have let us say this is the data, this is the data. Now, I am trying to predict the price of an apartment. Now, you know typically price of an apartment is dependent on multiple things, I have considered three such variables. This is the price of an apartment in lakhs of rupees, so this is the price of an apartment in lakhs of rupees. So, this apartment, apartment number one for example is 56 lakhs, so 5.68, yeah so it is 56 lakhs. This is 89 lakhs. This is 76 lakhs. This is 1 crore. This is 80 lakhs. This is 98 lakhs. This is 81 lakhs. So, it is in millions. So, 71 lakhs, 91 lakhs, 50 lakhs, 1.2 crores, 1.15 crore, those are the prices of the apartment. Those are the prices of the apartment. Now, it is our hypothesis that the price of an apartment will be dependent on its area, measured in square feet. So, this is 950 square feet, this is 1000 square feet, this is 870 square feet, this is 1000 square feet, this is 1145 square feet, there is an apartment which is 2000 square feet. So, these are the sizes of the apartment, these are the sizes of the apartment. Number of bedrooms. I am thinking that the prices will go up if I provide more bedrooms in the apartment. So, some apartments have two bedrooms, three bedrooms, some apartments only have one bedroom, some apartments have four bedrooms, so I am hypothesizing that the number of bedrooms may impact the prices. Now, there are families which have more than one cars. So, for some customers at least number of parking lots that are provided with the apartment may also matter. So, we are thinking that how many parking lots are offered may also impact the price of that apartment. So, some apartments if I buy an apartment, I will get two parking lots with that apartment. Sometimes they offer three parking lots without apartment. Sometimes they offer only one parking lot with the apartment and we are thinking that even that may impact the prices. So, we have data for the prices of apartments in rupees area measured in square feet. So, 9.5 means 950 square feet, 14.5 essentially means 1450 square feet, 11.45 means 1145 square feet, 15 means 1500 square feet, and so on. And we have data for 20 such apartments, we have data for 20 such apartments. Let us understand the correlation coefficient. Area apparently has a correlation of 0.46 positive which means as the area of the apartment goes up the prices go up. Number of bedrooms have a correlation of 0.6. So, as the number of bedrooms go up the prices go up, positive association not a trivial association. 0.6 is the correlation coefficient. And number of parking lots also seems to have a positive correlation with the prices. So, our hypothesis may be correct. Area seems to be impacting the prices of the apartment positively. Number of bedrooms seem to be affecting the prices of the apartment positively. Number of parking lots also seems to be impacting the prices positively. Where do I see that? If I run a simple linear regression, if I run a simple linear regression where area is the only explanatory variable. I am trying to predict the prices of the houses, prices of the apartment, prices of the apartment using only area as my explanatory variable, a simple linear regression. Look at the P value of my overall significance. Look at the P value of my overall significance, it is significant 0.03 is a good value. 0.03 helps me reject the null hypothesis that this regression is not significant. So, I will conclude that this regression is significant, if I only use number of bedrooms, and see the impact of that on prices. Once again, a simple linear regression this P value is once again very good, 0.004 which helps me reject the null hypothesis that this regression is not significant. So, I can conclude that even this simple linear regression is significant, this simple linear regression is significant, if I look at the impact of number of parking lots on the prices. Once again, the P value is 0.02, tells me that even this regression is a significant regression. So, by themselves each explanatory variable does help me predict the prices? What if I put all these explanatory variables together in my model, what if I put all these explanatory individually, they seem to be significant prior to predict prices, area turned out to be significant that P value was 0.03. Bedroom turned out to be significant that P value was 0.004. Number of parking lots also seem to be significant that P value was 0.02. If I put all of them together, if I put all of them together see what happens. Look at, if I put all of them together. Now, this is multiple linear regression, where I have looked at prices as my response variable and all the three variables were added as explanatory variables. Now, this one has 3 explanatory variables X1, X2, X3. R squared, very good. R squared of 0.49, very good. Multiple R, you know what multiple R means, multiple R means the correlation coefficient between the observed value of response variable and the predicted value of the response variable. 0.7 is good correlation, look at the overall significance, look at the overall significance, 0.01. What is this P value for? This is the P value for the null hypothesis, null hypothesis that the regression is not significant, regression is not significant, this is a null hypothesis. Now, look at this P value, P value of 0.01 essentially means that I reject this null hypothesis. So, the overall multiple regression is significant, overall multiple regression is significant just like each of the simple linear regressions were significant even the multiple linear regression model is significant, overall significance. We have found that it is overall significant. The funny part comes when you look at the individual beta values, look at the individual beta values. What is this P value correspond to? This P value corresponds to the very specific null hypothesis that particular beta, not beta 0, not beta 0, let us erase this. So, we are saying that this beta, this beta 1, area, beta 1 is 0 and alternate hypothesis that this beta 1 is actually not 0. Now, this P value of 0.7, P value is 0.7 which is much larger than my 0.05. So, we are saying that I cannot reject this null hypothesis, look at the confidence interval. For this particular explanatory variable this is the predicted value of beta 1, 0.05. So, this is my B 1, my predicted value of beta 1 is 0.05, 0.06 almost. But look at the confidence interval, it can be anywhere from negative 0.26 to 0.38, negative 0.26 to 0.38. So, if somebody says that you know what this beta 1 is actually 0, I cannot reject it because it is part of the confidence interval, I cannot reject it. Similarly, look at this P value, P value of 0.11. What does this correspond to? This corresponds to the null hypothesis that beta 3 is actually 0. Now, because of this large P value I cannot reject this null hypothesis. Again, look at the confidence interval, confidence interval is from negative 2.26 to positive 2.35 to positive 2.35, 0 is part of this confidence interval. So, if somebody says that this beta is actually 0, I cannot reject it, I cannot reject it. So, individually was area significant? Yes, in the simple linear regression area was significant, P value was only 0.03. Now, individually parking lots was significant, 0.11. Individually parking lot was significant. Where is my P value? P value was 0.02, but in the MLR parking lot P value is 0.11, telling me that parking lot beta is not significant. Why did this happen? This happened because of these things. As the area goes up it is quite likely that the builder will be able to provide more number of bedrooms. So, there is a fairly strong positive correlation between the area of the apartment and the number of bedrooms offered in the apartment. They are also seems to be a positive association between the area and the parking lot, 0.43. As the apartment is larger, more spacious apartment, those apartments also seem to be having more number of parking lots. So, the explanatory variables are correlated, they are correlated. And that is what is making these partial slopes insignificant even when the marginal slope was larger, marginal slope was larger. Now, look at this. What was the marginal slope? 0.32 and it was significant. P value was only 0.03. The estimate of beta 1 was 0.32. What is this estimate of beta 1? This is my marginal slope, this is my marginal slope. Now, look at what happens in the MLR. When I look at the secondary variable with other explanatory variables. This beta comes down, it is now only 0.05, this is partial slope. So, what was earlier 0.32 beta? Which was my marginal slope has come down drastically to only 0.06, come down by 0.06 and even there I do not know if somebody says it is 0, I will say, yeah probably it is 0. How much has been the inflation in variance? Inflation in variance is 0.145 here and 0.15 here. So, standard error seems to have gone up. So, the P value here reduced not because the standard error has gone up, but because the numerator has gone down drastically. So, if you actually calculate the variance inflation factor for each one of these variables. Variance inflation factor turns out to be 1.53, 1.34, 1.23, which means that the standard error estimation is getting inflated by some amount. And however, we are still not able to say it is a lot. So, how are we going to conclude about this VIF? How are we going to conclude? We saw in our example, in our housing pricing example, that the explanatory variables in the MLR are turning out to be insignificant. And why are they turning out to be insignificant? Because once we take into account the other explanatory variable, the unique explanatory power of the other explanatory variable is drastically reducing. So, what does this partial slope convey? Partial slope conveys the unique variance or the unique variation explained by that particular explanatory variable. Remember, what was the definition of our partial slope? Partial slope was the impact of this explanatory variable on the response variable keeping all the other explanatory variables constant. However, once you account for the other explanatory variable, once you account for parking lots, once you account for parking lots and area now the number of bedrooms does not offer anything unique that has all already not been explained by these two variables. Similarly, once you account for area and number of bedrooms, the parking lot does not offer anything unique whatever is impact of parking lots is already covered mostly in area and number of bedrooms, which is why the explanatory variables are turning out to be insignificant in the MLR but they are turning out to be significant in the simple linear regression. So, this is all happening essentially because of multicollinearity. So, this is the impact of multicollinearity. This is the impact of explanatory variables being correlated. So, what are the signs of collinearity? What are the signs of multicollinearity? We know that every time we add an explanatory variable, our R squared is going to go up. If there is multicollinearity R squared does not go up drastically, R squared goes up only fractionally. If that happens, let us say there is only one explanatory variable in our model, our R squared is 0.6. If we add one more explanatory variable, we expect the R square to go up drastically, but it goes up only from 0.6 to 0.65. So, that is a sign of multicollinearity. We saw this, the slopes of the correlate explanatory variables change drastically, we saw the marginal slope earlier was 0.32. If we add other explanatory variable marginal slope drops from 0.32 to become a partial slope of 0.06. So, this was a sign of multicollinearity. We also saw that the F-statistic is more impressive than the individual t-statistic. The overall significance of MLR was pretty good, we were able to reject the null hypothesis that the overall regression is not significant. However, individual t values, they turned out to be insignificant for a few explanatory variables. We also saw that the standard error for partial slope was larger than the partial error for marginal slope, because of the VIF. Since VIF was significant, we saw that the standard error was higher in the MLR than the standard error of partial slope in the SLR. So, these are all signs of multicollinearity. These are all signals that our regression model may have collinearity amongst the explanatory variables. Now, what are we going to do about it? What are we going to do about it? One thing we can clearly do is to remove redundant explanatory variables. These are all signs that tells us that our explanatory variables are correlated with each other, there is correlation amongst the explanatory variable. What are we going to do about this collinearity? How can we get rid of it? How can we reduce it? Can we get rid of it? One thing we can very easily do is to remove the redundant explanatory variables. So, what we can try and do is keep adding explanatory variable one by one. In the pricing of apartment example, we added all the three variables together. Perhaps we can try adding the explanatory variables one at a time, and possibly remove some of the redundant explanatory variables. The other way to deal with multicollinearity is to re-express our explanatory variables. What do I mean by re-expressing our explanatory variables? We can combine the correlated explanatory variables and create another explanatory variable which is a combination of these explanatory variables, do not include these correlated explanatory variables only include the new explanatory variable which is a combination of correlated explanatory variables. Typically, that is done in demographic studies. There are regression models where we want to include the demographic of our customer. For example, what is the monthly income of our customer? What kind of… how many vehicles does a person own? So, in general, for example, in general, we can say that, if the monthly income of the person is quite high, it is quite likely that the person has more than one vehicle, likely. I am giving a very broad example. So, instead Having these correlated explanatory variables, monthly income of the customer, number of vehicles owned by the customer, potentially they could be correlated, instead of that create a third variable, which is a combination of these two, and only include this third variable and do not include either the monthly income or the number of vehicles owned by the customer. So, we can re-express the explanatory variable and create a new variable and only include this combination variable in our regression model that can be tried. Otherwise, we do not have to really do something every time. If the explanatory variables are significant, and they have very, very sensible estimates, we do not have to remove them, we can keep them. What is an example of this? Earlier in our GPA example. In our GPA example, our explanatory variables were correlated entrance examination and interview was correlated, entrance examination interview were correlated, correlation of 0.54 positive. However, when we ran the MLR, when we ran the multiple linear regression model, in the multiple linear regression model, we had overall significance 0.0003 was the P value of the overall significance. So, overall significance of the regression model individually, the P values of the Betas were 0.019, which is 0.02 and 0.01. So, even here, the individual beta values were significant individual beta values were significant look at the confidence interval, confidence interval does not include 0. So, I will reject any null hypothesis that these beta values are 0, this beta value is not 0 nor this, neither is this beta value. So, the P values are significant, which means that the beta values are not statistically insignificant. So, I do not have to necessarily fiddle with the explanatory variables, if they are significant. So, even if there is collinearity, clearly there is collinearity because our explanatory variables are collinear, they are correlated. We may get lucky and we may not have to do anything about removing some of the explanatory variables. So, we have seen impact of collinearity, we have quantified the impact of colinearity through VIF. We have seen the impact of collinearity through path variables. And we have seen some of the remedies of getting rid of collinearity, we may not be able to get rid of collinearity, we may be able to reduce the collinearity by removing some of the redundant variables, but that may also be context based. So, let us end the session here and we will learn more about this in the tutorial that is going to follow. Thank you.