Hi all. Welcome to this lecture on implementing multiple linear regression in Python. I am S. Srivatsa Srinivas, a co-instructor at the IIT Madras online B. Sc degree program. We are going to implement multiple linear regression on Google collab. And we first import libraries, pandas and numpy for this. And the data set, which we are going to use has the GPA as the dependent variable, and the interview score and the entrance exam score as the independent variables. So, this is a hypothetical data set, which we are going to look at. You can click on this and there is an option to upload the file. So, we then read in this file with the sheet name being data. And this is how the header of the data set looks. Now, the variables of interest to us are Y, which is the dependent variable; and X1 and X2 which are the independent variables. Therefore, we create a reduced data frame. And the reduced data frame has Y, X1 and X2; as you can note here, since this is not of interest to us; we can very well drop that row. So, we are first dropping that row and we have revised data frame which looks like this. And the data points of interest to us are only till index 15; and these are some other calculations which were done in the sheet; so therefore, we need to remove all these as well. And we are removing that. And finally, we are renaming the Y variable as GPA X1 as entrance and X2 as interview. And only when you do an in place equal to true that will get reflected in the data frame df2. Now, we would need to look at how this data frame looks. So, all these values, GPA, entrance and interview are stored as objects. In order to perform numerical operations, we need to convert them into float. So, therefore we are converting these objects into float, all these three. And now this data frame will look like this; we have the GPA, the entrance, and the interview. And the shape of the data set will reflect on having 15 rows, that is the 15 data points and three columns. And since he had removed the first row, we would like to re-index this; so that we start from 0. So, we have 0, 1, 2, 3, 4 here and we would like to remove this column, which is the older index. So, we are filtering for only GPA, entrance and interview. Now, the first step in the linear regression in general is to look at the scatter plots of how different variables interact with each other. For that we first import the seaborn package. And then there is an option called pairplot, which will give us the scatter plot matrix. So, we can see with this on how a GPA is getting affected by entrance or interview, or the reverse; or how entrances is affected by interview and so on. So, all these are scatter plots of two variables in under consideration. So, you can look at that to see how the variables interact, which is having a linear relationship and so on. And then we are going to first implement the multiple linear regression using stats models. And then later, we will look at how to implement this multiple linear regression using sklearn. So, for first implementing this using stats models, we import these packages. And the way way you write a multiple linear regression equation, is to use the ordinary least squares regression which we are using now. And write the formula as the dependent variable which is the GPA and the independent variables as entrance and interview. This is how you write the expression for multiple linear regression. And when we execute this command, we can see that the R squared value is point 741. And since it is multiple linear regression, we have multiple independent variables; and we have adjusted R square of 0.697. And the P-value seems to be quite low, meaning that the regression is significant. And when we look at it here, we observed output very similar to what you see in Excel; that we have the coefficients of the independent variables, and the P-values associated with the independent variables. So, the P-value associated with entrance and interview are low enough; and therefore, these variables are going to be significant. As you would have noted in the lectures before, if some variable is not going to be significant; it might be because of multicollinearity. Therefore, it would be a good practice to add variables one by one, so that we can construct a significant linear regression with appropriate variables. So, the next step is to look at how do you calculate the variance inflation factor or the VIF. So, stats models has an option to do that. So, we first implement this that is stats models dot stats dot outliers underscore influence; and you import the variance inflation factor, so we do this. And I am also importing another package named add constant, and let you know how this works. So, if we are going to use this command to find the variance inflation factor of the two independent variables, entrance and interview. What happens is that this does not work appropriately; so, it is giving values of 70.71 and 70.71, which is not true. So, the way this has been designed is that you need to use the add constant term; that is you add a constant and only then apply the variance inflation factor, and that is when you get the right values. So, now when we do an add constant for both the entrance, interview here, and for the columns when we run over the loops, what we get is the following. So, this is for the constant which can be ignored; and these are the variance inflation factors for the two variables of interest, which is entrance and interview. So, as you have would have noted before, he need to keep this variance inflation factor under control. If the variance inflation factor goes out of control, then the standard error will go out of control for that particular variable. And as a consequence, what you will observe is that the P-value will become high. So, when the P-value becomes high, what will you do is you will fail to reject the null hypothesis; that is, beta1 is equal to 0, meaning that this variable will not be significant. Only when you keep these various inflation factors under control, you can then ensure the significance of the independent variables. And therefore, variance inflation factor helps us in quantifying the effect of the collinearity between the independent variables. Now, we may want to look at how the residuals look. So, we create a new data frame df4, and this results to dot fitted values dot copy will give us the predicted values, and the observed values are the GPA. And when we find observed minus predicted, we obtain the residual and this is how it looks. And if we want to calculate the root mean square error of the regression model, what we can do is to square these residuals; and sum these residuals over all the observations, divided by the total number of observations and finally, find a square root. So, this was the mean squared error and when we find the square root, it is the root mean squared error; and we find that it is about 0.56. So, this is how the root mean squared error value looks. So, this is how you implement multiple linear regression using stats models. You can do the same using sklearn as well. So, for that we first import linear model from sklearn, and then we also import the mean squared error and r2 score from sklearn. So, this regmodel2 will give us the linear regression model. And the variables of interest to us are the entrance and the interview as independent variables, and GPA as the dependent variable. As you can see here, when I perform the reshape operations, since there are two independent variables here, we enter the value of 2. If instead we had three independent variables, he would be entering a value of 3 here. So, what we require is a Y-value that is the GPA or the dependent variables value, and correspondingly all the independent variable values. And therefore, this reshape operation is required; so, now I run this and fit the model. And as you can see here, the coefficient of the model will match with the one we found using stats models; this 0.4554 and 0.6225 that is what we obtained here. And similarly, the intercept values will also match. And the next step which we do is to find the predicted values of these observations using the regression model, which we have built, which has an intercept of minus point seven, and the coefficients as follows. So, when we run this, we obtain the predicted values. So, we are storing in the predicted values here. So, as you can see here, the observed value is 9.5, the predicted value is 9.42, observed value is 6.3, the predicted value is 7.13 and so on. If you go back to stats models, that is what you would see. So, the predicted value, there was 9.42, the observed value was 9.5, predicted value was 7.13, and the observed value was 6.3. So, you can clearly see that both sklearn and stats models are consistent in a way. Now, how do you update the root mean squared error and R squared values, quite straightforward? You use the mean squared error which we had imported, and then we would like to find it between the predicted values which is this, and the observed values which is this. And similarly, we obtained the R squared as follows with r2 underscore score and this is how it will look. We have the root mean squared error here and the R squared value here. And you can very well see that it will match with what he had observed before. So, the RMSE value was about 0.56; and the R squared value is 0.741. Now, we also have the value of the adjusted R squared directly from stats models. But, there is no direct package to obtain it here. So, what we do is we use r2 score to calculate adjusted R square; and that is 1 minus r square which is the R squared value for GPA and the predicted value; and we multiply it by the number of observations which is n minus 1. Then, we divided by n minus the number of independent variables minus 1 and we obtain the following. So, we have obtain 0.697, which matches with the adjusted R squared value observed earlier. So, this is how you implement multiple linear regression on Python using a either stats models or sklearn. Thank you.