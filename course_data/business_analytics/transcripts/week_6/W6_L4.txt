Welcome back to this session on multiple regression. In the previous session we have seen how path diagram helps us understand the direct and indirect effect of an explanatory variable on the response variable and as we saw the path variables are more relevant when the explanatory variables are correlated. Today, we are going to extend that discussion and talk about one more quantification of this collinearity. The relationship between the explanatory variables that particular situation is referred to as collinearity or multicollinearity. And we have seen the effect of collinearity through path diagram. Today, what we are going to do is look at what is called as look at what is called as variance inflation factor VIF, which is also a quantification of collinearity amongst the explanatory variable. So, first of all, what is variance inflation factor? Variance inflation factor is the amount of unique variation in each explanatory variable and it essentially measures the effect of collinearity. So, in particular this VIF for a particular explanatory variable is calculated as 1 over 1 minus R squared, where this R squared is not the regular R squared of the multiple linear regression, but this R squared is the coefficient of determination on a special regression where that particular explanatory variable is the response variable and all the other explanatory variables are the explanatory variables. What do I mean by that? So, let us say that there is a multiple linear regression where the explanatory variables are X1, X2, X3, X4 and we are trying to study the impact of these explanatory variables on the response variable Y. Now, what will be VIF of X1? VIF of X1 will be 1 over 1 minus R1 squared. What is this R1? R1 or R1 squared R1 squared is the coefficient of determination in a regression where X1 is the response variable and X2, X3, X4 are your explanatory variables. When we calculate VIF of X2 we will say it is 1 minus R2 squared, and what is this R2 squared? R2 squared comes from the regression where X2 is the response variable, X2 is the response variable and X1, X3, X4 they are our explanatory variables. So, now, what will happen if this R squared is a large value? When will the R squared be a large value? R squared will be a large value if this regression is significant. Which means that explanatory variable X1 is fairly correlated with X2, X3, X4. If that happens, then this R squared will be a large value. If this R squared is a large value VIF will be a large value. So, this is how you quantify variance inflation factor now, why is it called variance inflation factor? Now, if you recall, if you recall we discussed about the, we discussed about the partial slope and the marginal slope. Now, when we discussed about the partial slope and the marginal slope, what is the explanation for this estimation of the partial slope? So, partial slope we discussed that it is beta naught plus beta 1 X1 plus beta 2 X2 and so on. So, this beta 1, beta 2 are the partial slopes. Now, from the sample of data on which we are going to run the regression, I am going to get an estimate of beta 1 which we called as b1 from the data I am going to get an estimate of beta 2 which is b2. Now, this is only an estimate and therefore, it is going to have a standard error of itself, standard error in estimating beta 1 can also be calculated and if you had noticed in the excel output this standard error is also getting recorded. So, similarly, there is going to be a standard error in predicting beta 2 which is called SE of b2 a standard error in b2 standard error in b1. Now, what was the explanation for this standard error? So, first of all let us first of all see where is this getting recorded? So, let us go to excel of our GPA example that we had discussed last time, GPA example of what we had discussed last time where remember. We were looking at we were looking at we were looking at CGPA in the MBA programme as our response variable CGPA in the MBA programme as our response variable the scores in the entrance examination and scores in the interview were our explanatory variables. Recall that regression in the previous session, so, here we had said that the estimate of beta 1 is 0.455 and the estimate of beta 2 is 0.622. Now, Excel also reported the standard error in this estimation standard error reported was 0.168 and 0.213. So, this is essentially this value, this value is essentially standard error in estimating b2, this value is essentially standard error in estimating b1. So, how are these calculated? So, this is where it is reported, if we had seen the simple linear regression where we are where we had considered only one of the explanatory variables this was our standard error in estimating the beta. So, this was our SE b1 and similarly, this would have been our standard error in estimating beta 2. So, this is what we are talking about. This is what we mean by standard error in estimating the slopes. How are the standard errors calculated? Standard error in estimating b1 is generally given by the standard error which you already know what this is. this is an estimate of sigma of epsilon divided by square root of n multiplied by 1 over standard deviation in x, what is standard deviation in x? This will be standard deviation in X1. So, let us go to that variance inflation factor. So, generally the standard error in b1 is estimated as standard error of the standard error in the error terms divided by the square root of n 1 over standard deviation of that particular explanatory variable. So, if you are estimating standard error in b1 this will be standard deviation of X1. If you are if you are calculating the standard deviation in b2 this will be standard deviation of X2 variable. Now, we all know why standard deviation in X2 is in the denominator. If the X1 range is quite large, if the X1 range is quite large, which means that the standard deviation of X1 is quite large that actually helps me understand the variation in Y and therefore, if the standard deviation in X is quite large, the standard error in the corresponding beta value will be smaller and what is what do I mean by this standard error value being smaller I get very high precision in estimating that particular beta value. Once again take the extreme example what if all the X values, all the X values were same 1, 1, 1, 1, 1, 1 and therefore, the standard deviation of this will be 0. If the standard deviation is 0 what will happen to the standard error of b1 standard error of b1 will skyrocket, which means that you will get absolutely no precision in estimating that particular beta. So, this is typically what happens in absence of VIF in absence of VIF. When will VIF be absent? VIF will be absent if the explanatory variables are uncorrelated. But if there is VIF if this is if there is VIF, the standard error in the estimation of b1 gets inflated and how does it get inflated? It gets inflated by this factor. So with VF, the standard error in estimating b1 is actually much more standard error in estimating b1 is actually much more by this much amount by standard deviation of VIF amount. Now, going further, how is this VIF? So, if the explanatory variables are completely uncorrelated, if the explanatory variables are completely uncorrelated, then the coefficient of determination in that special regression would be 0 and if you plug in 0 here, you will get a VIF of 1. Now, VIF of 1 if you plug it in here, which means that there is no change in our precision, the standard error of b1 remains pretty much the same if VIF is actually 1, when will VIF be 1 VIF will be 1 if the correlation amongst the explanatory variable is just absent when we ran that spatial regression where one of the military variable is made response variable, if that regression has an R squared of 0, then the VIF will be 1. However, if the explanatory variables are correlated somehow and VIF turns out to be a value more than 1, then we can say that there is collinearity in our model and as we saw larger value of VIF essentially increases the standard error in predicting the partial slope and therefore, it can make our predictions very very unreliable. Now, let us look at let us look at our data, let us look at our data. Let us, go back to that GPA example. So, this was the, this was the multiple linear regression model and if you recall from the data, the explanatory variables were correlated explanatory variables were correlated. Entrance examination was an explanatory variable interview was an explanatory variable and there was a 54 percent coefficient of correlation, a 0.54 was the coefficient of correlation between the two explanatory variables. How does that get reflected? That gets reflected by running a regression where you make one of the explanatory variables are response variable the other explanatory variables remains as an explanatory variable and you see that the R squared is almost 0.3 0.29 is the R squared, this is the R squared that is going to get used in calculating the VIF. Let me say that again. What is this special regression? This is a special regression, there was a simple linear regression where we had used one of the explanatory variable against the response variable. Here, the response variable was the CGPA in college and one of the explanatory variables was kept in the model. In the second simple linear regression, our response variable did not change our response variable was CGPA during the MBA programme, the explanatory variable change. Now, this is a special regression, this is a special regression where we have made one of the explanatory variables were one of the original response original explanatory variables as a response variable and the other explanatory variable remains as an explanatory variable. So, the R squared reported was 0.29. And therefore, I can calculate the variance inflation factor this 0.29 is the R squared. Similarly, the other regression is also going to report the same 0.29. Where it does not matter whether I have the entrance examination as the explanatory variable or whether I have the interview as an explanatory variable the VIF is still going to be 0.29 and therefore, the VIF is going to be 1.41 and the square root of this is going to be the inflation. So, we can say from this value that there is going to be an 18 percent increase in the standard error of the corresponding beta value. So, going back to this expression here, corresponding this is a square root of VIF turned out to be 1.18 and therefore, the standard error in beta 1 is going to increase by about 18 percent. Similarly, the standard error in estimating beta 2 is going to increase by 18 percent. Now, fortunately for us in our example that we have taken for our example, the inflation because of VIF was not much it was only 18 percent increase it was only 18 percent increase. However, sometimes the VIF could be very large. For us, we were a little more fortunate, that our R was only 0.54 and particularly the R squared, the R squared was 0.29 was only 0.29. Now, imagine if this R squared was of the range of 0.7, for example, 0.7. Now, if this R squared was 0.7. Let us, see what would have happened. The variance inflation factor would have been 3.33, variance inflation factor would have been 3.33. Therefore, the square root of that 1.82 there would have been 82 percent increase, there would have been an 82 percent increase in the standard error of b1. What does this do? Why do I want to keep the standard error in estimating beta 1 to be small? In general, because it is standard error anywhere I see standard error in regression, I want to keep it to the minimum. Now, what will happen if this standard error gets inflated, which is why we are referring to this as VIF it is variance inflation factor, what if this standard error in estimating beta 1 gets inflated? Look at the multiple linear regression model. Now, let me delete this so that this becomes clearer. Now, if this if the standard error terms get inflated and for us, we were fortunate that standard error did not get inflated by 82 percent. The error the standard error inflation was quite small only 18 percent. If this would have been very high, what would happen to the t statistic the t statistic would come down. Why would t statistic come down? T statistic is how is the t statistic calculated? That t statistic is calculated like this. How is this t statistic calculated? This t statistic is for our null hypothesis that that particular beta value is 0 and how is this calculated, this is calculated as the prediction predicted value of that beta divided by the standard error of that beta. Now, if this standard error gets inflated, because of VIF, now, because of VIF, let us say the standard error gets inflated this t value is going to reduce. This t value is going to come down and what if this value comes down? It may actually impact my P value. If this t statistic is very small. I may not be able to reject this hypothesis, I may not be able to reject this null hypothesis. What if I am not able to reject this null hypothesis? If I am not able to reject this null hypothesis, I may end up saying that well I do not know this beta could be 0, I cannot say for I cannot say with confidence that this beta is not 0 I am not able to reject this null hypothesis. And what if I am not able to reject this null hypothesis? If I am not able to reject this null hypothesis, it means that that particular explanatory variable may be statistically insignificant for the regression, that particular explanatory variable may turn out to be insignificant for the regression that is really the extreme case of collinearity. I will discuss another example for this, I will discuss another example for this where I will demonstrate an extreme case. But, coming back to this, I do not want this explanatory to be insignificant in my regression. Therefore, I do not want this t value to be small. Therefore, I do not want this standard error to be a large value. If I do not want this standard error to be a large value, I better make sure that the VIF is in control and the only way to make VIF in control is to ensure that the explanatory variables do not have too much correlation, is that point understood?