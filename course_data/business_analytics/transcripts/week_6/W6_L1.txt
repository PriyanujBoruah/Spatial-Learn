Welcome to this new session. As the title mentions, today we are going to discuss a simple application and example of Multiple Regression. Multiple regression is a technique which can include multiple explanatory variable. So, this becomes an extension of the last topic, which was simple linear regression, where we had one explanatory variable and one dependent variable, one response variable as it is called. In multiple linear regression, we are going to have multiple explanatory variable and a single response variable, a single dependent variable. So, essentially, we are going to study the effect of each of these explanatory variables on the response variable. But, we are going to see some interesting interaction effect. So, particularly when explanatory variables are not really independent of each other. So, the interesting insights are particularly in that space. So, as we had seen the expression for simple linear regression, let us extend that. And now we will have ‘k’ such explanatory variable ‘k’ was 1 in the simple linear regression model. So, now we can call it as either MLR or MRM does not matter what acronym we use, Multiple Linear Regression or Multiple Regression Model. We are going to explicitly say that, right now, we are interested only in the linear regression. Let us focus on linear regression. So, that we can draw those insights as I had mentioned earlier. So, for a simple, for a multiple linear regression, the expression for the dependent variable is going to be of this type, where Y is our dependent variable Y is our response variable and X1, X2, X3, Xk are all the explanatory variables and of course we have the same epsilon term the error term. Just like a simple linear regression the error term is going to have some assumptions. So, we are going to make some assumptions about the error term. In particular we are going to assume that the error terms are independent of each other. We are going to assume that the error terms have equal variance and the variance is actually sigma epsilon squared and that the error terms are normally distributed. These were the same assumptions that we had on the simple linear regression error term also. So, the question becomes estimation of these beta variables beta not, beta 1, all the way to beta k. So, there are k plus 1 beta’s to be to be estimated. And we are only going to have a sample of values for X1, X2, X3, all the way to Xk as well as Y. And from this sample of data, we are going to estimate these parameters, beta not, beta 1, beta 2, beta3 all the way to beta k. So, this is this is the multiple regression model. All the things that we had said about a simple linear regression apply, which means that the expected value of Y given the bunch of X will be equal to beta naught plus beta 1 X1 plus beta 2 X2, all the way to beta k Xk. So, all also the expected value is going to be beta not all the way to beta k beta k xk. Because, the expected value of epsilon is going to be 0. Because the error term has 0 mean. So, essentially the main difference between a simple linear regression model and multiple regression model is that there is only one explanatory variable in simple linear regression which means that the equation is of this type. So, beta naught plus beta 1 X1 plus epsilon and then the multiple regression will have beta 0 beta 1 X1 plus beta 2 X2, all the way to beta k Xk plus epsilon. Now, the impact of all the other exponentially variable, which was not explicitly considered in the simple linear regression, will get lumped into the error term. So, in general this error term will incorporate, will consider the effect of all these variables which were not considered in the simple linear regression model. So, this was our simple linear regression model, this was our multiple regression model. So, this is going to be the main difference. So, whatever effect was not explicitly considered will get into the error term. Now, so let us look at the numbers and that will tell us a few more things. There are couple of more things that we are going to specifically focus on. One of them is called adjusted R squared. If you remember in the simple linear regression, output of excel, we said the excel prints something like adjusted R squared. But. we will consider it only when we look at multiple regression. So, this is the time we should consider that. So, in general if you add explanatory variables to the regression the R squared is expected to go up. However, it should go up purely based on the explanatory power of the model, it should not go up, because we are adding explanatory variables. So, adjusted R squared adjusts for that number of explanatory variables that we have in the model. In general, it is expected to be smaller than the R squared that is reported. So, adjusted R squared gives us a slightly more realistic picture of what is the combined explanatory power of all these explanatory variables that we have in the model. Adjusted R squared, which is generally I have denoted that by R bar squared and R squared is our typical coefficient of determination. R bar squared also adjusts for the sample size. There is a specific expression for R bar square, the adjusted R squared. But, let us not worry about that. So, just like simple linear regression model, we were focused on R squared value and ‘S e’ value, which is estimate of sigma epsilon. Sigma epsilon is what we want, but from the sample we are only going to get S e as a standard deviation of the error terms. Those two are going to help us understand, generally speaking, whether our regression model is good or not. In general, we want a large value of R squared or adjusted R bar squared, we want the larger value of that and we want a smaller value of ‘S e’, we want a smaller standard deviation of the error terms. And we can see from the multiple regression model that R bar squared and ‘S e’ squared move in the opposite direction when we keep adding exponential variables to the model. In general, the our adjusted R squared goes up whereas the standard deviation of the error term generally comes down, if you add explanatory variables to the model. So, the one way to understand this impact of R squared is to look at actually the scatter plot between the observed values of the Y variable and the fitted values of the Y variable. The big difference between the interpretation of R in SLR against MLR is that actually, our R value in general, if we add explanatory variables to the multiple regression model, the R squared value is expected to go up and the standard deviation of the error term is expected to come down. Now, this R itself, the coefficient of correlation is a tricky thing in multiple regression model. In a simple linear regression model, we said R represents the correlation between X and Y because there was only one X and obviously one Y. In multiple regression model, there are multiple X's, multiple explanatory variables. So, R should represent what? So, in multiple regression model actually R represents the correlation between the observed value of the response variable and the fitted value of the response variable. So, more tighter this relationship, is the higher should be the R value. So, that is the interpretation of R in multiple correlation. Now, the slope, let us go back to the expression, here the slope of the line, which is beta 1, is called the ‘marginal slope’. What is marginal slope? Marginal slope is the change in Y variable with one unit change in the X variable. There is no other explanatory variable. So, this definition is enough. Look at the same coefficient of X1 in the multiple regression model. This coefficient is called ‘partial slope’. This is called marginal slope. This is called partial slope. How do we define beta 1 in a multiple regression model corresponding to the same explanatory variable? How do we define beta 1 here? We say that beta 1 represents the change in Y variable with one unit change in X variable, keeping all the other X variables constant, which means suppressing the effect of all the other explanatory variable. What is the impact of this explanatory variable on Y? That is the interpretation of this beta 1. Even though the term used is same, here also we are calling it as beta 1 X1, here also we are calling it as beta 1 X1, this beta 1 is called marginal slope, this beta 1 is called partial slope in MLR. Now, ideally speaking, we want exponential variables which are orthogonal to each other, which are independent of each other. If this is truly correct if the explanatory variables that we have in the regression model, are truly independent of each other, then the marginal slope and the partial slope will have the same value. Rarely however this happens. Rarely the explanatory variables are completely independent of each other. In that case, the marginal slope and the partial slope may differ in values. So, that is the interpretation of partial slope and marginal slope. So, once again, what is partial slope? Slope of the explanatory variable that statistically excludes the effect of other explanatory variables, that is what we said, keeping all the other X variables constant, what is the change in Y with one unit change in this explanatory variable. So, marginal slope however is the slope that comes from the simple linear regression. As we said the marginal slope and partial slope will coincide if the explanatory variables are really independent. Rarely however it happens in most of the data sets. Now, this path diagram. What if the explanatory variables are not independent? Then actually, that situation is called co-linearity, you are already aware of it from your previous courses. Co-linearity is a situation which represents very high correlation amongst the explanatory variable. And sometimes that the colinearity may be so severe that it actually makes the estimates of MLR very difficult to interpret and we will see an example of that also. But, what is this path diagram? Path diagram, essentially, so, what are we saying, in multiple regression model there is an explanatory variable X1 which has an impact on Y, there is an explanatory variable X2 which has an impact on Y, there may be an explanatory variable x3 which has an impact on Y, maybe one more x4 which has an impact on Y. Ideally, we are saying that these explanatory variables are independent of each other. What if that is not true? So, path diagram indicates the relationship amongst the explanatory variable and the response variable. So, in addition to these arrows, what if X1 may also influence X2. Because, X1 and X2 are not independent X1 may influence X2. So, now, we can say that X1 impacts Y in two different ways, there is a direct effect of X1 on Y which is this arrow. Now, there is an indirect effect of X1 on Y. Because X1 impacts X2 and in turn X2 impacts Y. So, this path is called the indirect effect of X1 on Y. If, once again let me state it, if X1 and X2 were independent, if this path, if this arrow was statistically not significant, then the indirect effect of X1 on Y will be 0. However, if X1 and X2 are correlated, X1 may influence Y directly, X1 may influence Y and X1 may influence X2 and X2 may influence Y. So, that may also happen. So, what is the total effect of X1 on Y? The total effect of X1 on Y is the direct effect which is this. Plus, the indirect effect. And this relationship should hold. I should be able to show that if I add up the direct effect and if I add up the indirect effect, I get the total effect of X1 and Y. Where will I get the total effect of X1 on Y? The total effect of X1 on Y is represented in the marginal slope. It is represented in the marginal slope, because there was no other explanatory variable. So, if I say that Y is equal to beta not plus beta 1 X1. This is the total effect of X1 on Y. Whereas, when I say beta not plus beta 1 X1 plus beta 2 X2. Now, there is a direct effect of X1 on Y. Now, X1 and X2 may be correlated. X1 and X2 may be correlated and some impact, some amount of this beta 2 may be because of X1. Some amount, not directly, not all beta 2. So, some amount of this beta 2 may be because of X1. So, this is called the indirect effect, some component of beta 2 influencing Y is the indirect effect. So, this total effect should match the direct effect plus the indirect effect. Right now, this will look like slightly abstract idea, but let us take an example. The best thing is to take an example So, let us stop the PPT. Let us stop the ppt and actually go to the excel sheet, where I have the example. So, this is the data.